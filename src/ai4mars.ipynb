{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI4Mars\n",
    "\n",
    "A Machine learning Model for Martian Terrain Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The objective of this project is to take the amazing work done by NASA and train a machine learning model on their [AI4Mars](https://data.nasa.gov/Space-Science/AI4MARS-A-Dataset-for-Terrain-Aware-Autonomous-Dri/cykx-2qix/about_data) dataset. This dataset is the culmination of an incredible effort by experts and the public at large to create a dataset for semantic segmentation of Martian terrain.\n",
    "\n",
    "The AI4Mars dataset consists ~326K semantic segmentation full image labels on 35K images from Curiosity, Opportunity, and Spirit rovers, collected through crowdsourcing. Each image was labeled by 10 people to ensure greater quality and agreement of the crowdsourced labels. It also includes ~1.5K validation labels annotated by the rover planners and scientists from NASA’s MSL (Mars Science Laboratory) mission, which operates the Curiosity rover, and MER (Mars Exploration Rovers) mission, which operated the Spirit and Opportunity rovers.\n",
    "\n",
    "As opposed to Earthly applications, training machine learning models for deep space missions is incredibly difficult. This is largely due to the scarcity of available training data and the stringent requirements for safety-critical flight software. As such, this project aims to train a machine learning model under the constraints imposed by a typical deep space mission. This means that efficiency in the size and resource utilization of a machine learning model will be considered a requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "| ID          | Description                                                                         | Rationale                                                         | Verification Method |\n",
    "|-------------|-------------------------------------------------------------------------------------|-------------------------------------------------------------------|---------------------|\n",
    "| AI4MARS-001 | A machine learning model shall be trained on the AI4Mars v0.1 dataset                    | The AI4Mars dataset is robust and high quality.                   | Inspection          |\n",
    "| AI4MARS-002 | A machine learning model shall perform semantic segmentation of Mars terrain imagery| The task of the machine learning model is segmentation.           | Testing             |\n",
    "| AI4MARS-003 | A machine learning model shall execute in under 2GB of RAM                          | The allocated RAM budget for the machine learning model is 4GB.   | Inspection          |\n",
    "| AI4MARS-004 | A machine learning model shall fit in under 500MB of disk storage                     | The allocated disk budget for the machine learning model is 1GB.  | Inspection          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Lets start with setting up our notebook. We'll need to import our dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._custom_ops'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import all the modules we will need\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimports\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mall\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ai4mars/lib/python3.11/site-packages/fastai/vision/all.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mall\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/ai4mars/lib/python3.11/site-packages/fastai/vision/models/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m xresnet\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unet\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/ai4mars/lib/python3.11/site-packages/fastai/vision/models/tvm.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_t\u001b[39;00m\n\u001b[1;32m      4\u001b[0m _g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/ai4mars/lib/python3.11/site-packages/torchvision/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodulefinder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/ai4mars/lib/python3.11/site-packages/torchvision/_meta_registrations.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_custom_ops\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlibrary\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Ensure that torch.ops.torchvision is visible\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch._custom_ops'"
     ]
    }
   ],
   "source": [
    "# Import all the modules we will need\n",
    "from fastai.imports import *\n",
    "from fastai.vision.all import *\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import PIL\n",
    "\n",
    "from utils.utils import *\n",
    "\n",
    "torch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "# ! We need this to get some of the training output to work. This will be fixed\n",
    "# in a future release of Jupyter for VS Code.\n",
    "# https://github.com/microsoft/vscode-jupyter/pull/13442#issuecomment-1541584881\n",
    "from IPython.display import clear_output, display, DisplayHandle, Image\n",
    "\n",
    "\n",
    "def update_patch(self, obj):\n",
    "    clear_output(wait=True)\n",
    "    self.display(obj)\n",
    "    \n",
    "# Enable Weights & Biases if the env variable is set\n",
    "# if os.getenv(\"ENABLE_WANDB\"):\n",
    "if True:\n",
    "    g_ENABLE_WANDB = True\n",
    "else:\n",
    "    g_ENABLE_WANDB = False\n",
    "\n",
    "if g_ENABLE_WANDB:\n",
    "    os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"ai4mars.ipynb\"\n",
    "    import wandb\n",
    "    from fastai.callback.wandb import *\n",
    "    wandb.login()\n",
    " \n",
    "DisplayHandle.update = update_patch\n",
    "\n",
    "g_DEVICE = enable_gpu_if_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start logging a wandb run\n",
    "if g_ENABLE_WANDB:\n",
    "    wandb.init(project=\"AI4Mars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Data\n",
    "\n",
    "Now that we have all of our packages imported and our notebook set up, we can\n",
    "proceed with downloading our data from Kaggle. \n",
    "\n",
    "Note that the data is available directly from NASA, a user-uploaded version to Kaggle makes downloading that data much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"yash92328/ai4mars-terrainaware-autonomous-driving-on-mars\"\n",
    "dataset_path = URLs.path(dataset_name)\n",
    "\n",
    "Path.BASE_PATH = dataset_path\n",
    "\n",
    "# Download the dataset to a hidden folder and extract it from kaggle\n",
    "if not dataset_path.exists():\n",
    "    import kaggle\n",
    "\n",
    "    dataset_path.mkdir(parents=True, exist_ok=True)\n",
    "    kaggle.api.dataset_download_cli(dataset_name, path=dataset_path, unzip=True)\n",
    "\n",
    "# Lets append the subfolder to our path\n",
    "dataset_path = Path(dataset_path / dataset_path.ls()[0])\n",
    "\n",
    "dataset_path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the paths to our images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "IMAGES_PATH = Path(dataset_path / \"msl\" / \"images\" / \"edr\")\n",
    "MASK_PATH_TRAIN = Path(dataset_path / \"msl\" / \"labels\" / \"train\")\n",
    "MASK_PATH_TEST = Path(\n",
    "    dataset_path / \"msl\" / \"labels\" / \"test\" / \"masked-gold-min3-100agree\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(IMAGES_PATH.ls()), len(MASK_PATH_TRAIN.ls())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we'll notice is that, for some undocumented reason, there are some images missing their labels. We'll need to make sure all our training images have their labels next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_missing_labels = find_images_missing_labels(IMAGES_PATH, MASK_PATH_TRAIN)\n",
    "# len(images_missing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path(dataset_path / \"archive\").mkdir(exist_ok=True)\n",
    "\n",
    "# for img_path in images_missing_labels:\n",
    "#     shutil.move(img_path, Path(dataset_path / \"archive\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(IMAGES_PATH.ls()) == len(MASK_PATH_TRAIN.ls()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "\n",
    "Now that we have the data downloaded, we can proceed with actually visualizing some of the images and masks that we have.\n",
    "\n",
    "Lets start by first seeing what one of these images looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PILImage.create(IMAGES_PATH.ls()[2])\n",
    "img.show(figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to see the corresponding mask associated with our image, we'll need a small function to help us map them on\n",
    "the fly. According to the AI4Mars info.txt file, the images end with extension `.JPG` while the corresponding label\n",
    "ends with `.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mask_path = lambda file: MASK_PATH_TRAIN / f\"{file.stem}.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_mask = PILMask.create(get_mask_path(IMAGES_PATH.ls()[2]))\n",
    "example_mask.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In semantic segmentation, the “labels” are a 1:1 mask of the original picture with each pixel representing a label and are single channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor(example_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very quickly we see an issue!\n",
    "\n",
    "Because of how the loss gets calculated (and how fastai does things in general), the values of the pixel mask must be from 0 -> n, with n being the number of classes possible. If we take things as they are here during training you’ll hit an error that says “CUDA Segmentation Fault, Index Out of Bounds” (or something similar).\n",
    "\n",
    "This is because our labels should be from 0 -> 4, to align with the fact predicted probabilities from our model are 0 -> 4. Instead they are 0, 1, 2, 3 and 255, leading to this issue.\n",
    "\n",
    "So how do we fix the issue? In numpy we can just override the numbers for a particular value in the array and set it. To generalize this however a dictionary of the original value to the new one should also be made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_codes = {\n",
    "    0: 0,\n",
    "    1: 1,\n",
    "    2: 2,\n",
    "    3: 3,\n",
    "    4: 255\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create a get_y function. In this case it should take in a filename and our dictionary, open the filename, and return the mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(filename: Path, unique_codes: dict) -> PILMask:\n",
    "    filename = get_mask_path(filename)\n",
    "    mask_array = np.asarray(PIL.Image.open(filename)).copy()\n",
    "    \n",
    "    mask_array[mask_array == 255] = 4\n",
    "    \n",
    "    return PILMask.create(mask_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets stop check this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor(get_label(IMAGES_PATH.ls()[2], unique_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask: PILMask = get_label(IMAGES_PATH.ls()[2], unique_codes)\n",
    "mask.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we should do is create a list of our codes that map to the corresponding pixel value. According to the\n",
    "info.txt, the codes are as follows:\n",
    "\n",
    "| RGB         | Key             |\n",
    "|-------------|-----------------|\n",
    "| 0,0,0       | soil            |\n",
    "| 1,1,1       | bedrock         |\n",
    "| 2,2,2       | sand            |\n",
    "| 3,3,3       | big rock        |\n",
    "| 255,255,255 -> 4, 4, 4 | NULL (no label) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = np.array([\"soil\", \"bedrock\", \"sand\", \"big rock\", \"null\"], dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is in place to create our `DataBlock` object. Jeremy Howard, the founder of Fast.AI, popularized the idea of image resizing:\n",
    "\n",
    "* Train on smaller sized images\n",
    "* Eventually get larger and larger\n",
    "* Transfer Learning loop\n",
    "\n",
    "In the AI4Mars paper, the authors mention that they resize their images to 512x512 from the original 1024x1024 size.\n",
    "\n",
    "Since I am training on a laptop GPU, for this first round we will train at half the image size as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_sizes = mask.shape; mask_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_sizes = tuple(int(x / 2) for x in mask_sizes); mask_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Our `DataLoader`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the AI4Mars paper, the authors mention that the \"batch size was chosen to be as large as possible before running into GPU memory issues\". We will do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataBlock(\n",
    "    blocks=(ImageBlock, MaskBlock(codes=codes)), # our input is an image and outpus is a mask\n",
    "    splitter=RandomSplitter(), # randomly split our dataset into 80% train and 20% valid\n",
    "    get_y=partial(get_label, unique_codes=unique_codes), # load our preprocessed labels\n",
    "    batch_tfms=[*aug_transforms(size=mask_sizes), Normalize.from_stats(*imagenet_stats)] # apply some standard augs\n",
    ").dataloaders(get_image_files(IMAGES_PATH), bs=8)\n",
    "\n",
    "data_loader.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Our Model Architecture\n",
    "\n",
    "Next, we can create our `Learner` object that will wrap our model architecture, hyperparameters, and `DataLoader` into one abstract object.\n",
    "\n",
    "In the AI4Mars paper, the authors opted for the DeepLabv3+ model architecture with a ResNet-101 backend pretrained on ImageNet. For our architecture, we will go with a U-Net with a ResNet18 backing that's been pre-trained on ImageNet. We _could_ utilize a fancy modern vision transformer, however, with spacecraft you want technology that is tried and true. U-Net has a long and rich history that make it a reliable model architecture for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CamVidAcc(input: torch.Tensor, target: torch.Tensor, axis: int=1) -> torch.Tensor:\n",
    "    \"\"\"For segmentation, we want to squeeze all the outputted values to\n",
    "    have it as a matrix of digits for our segmentation mask. From there,\n",
    "    we want to match their argmax to the target's mask for each pixel\n",
    "    and take the average\n",
    "\n",
    "    Args:\n",
    "        input (torch.Tensor): The independent variable\n",
    "        target (torch.Tensor): The dependent variable\n",
    "        axis (int, optional): The dim to apply argmax to. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The mean of the input argmax to the target's mask\n",
    "    \"\"\"\n",
    "    target = target.squeeze(1)\n",
    "    mask = target != 4\n",
    "    return (input.argmax(dim=axis)[mask] == target[mask]).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if g_ENABLE_WANDB:\n",
    "    learner = unet_learner(\n",
    "        data_loader,\n",
    "        resnet18,\n",
    "        metrics=[CamVidAcc(), JaccardCoeff(), Dice()],\n",
    "        act_cls=Mish, # use a modern activation function\n",
    "        loss_func=DiceLoss(axis=1),\n",
    "        self_attention=True,\n",
    "        opt_func=ranger # use the Ranger optimization function\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what our model architecture looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = learner.lr_find(suggest_funcs=(minimum, steep, valley, slide))\n",
    "\n",
    "learner.lr =  learning_rates.valley\n",
    "\n",
    "f\"Learning Rate: {learner.lr}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets fine tune our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_gpu_cache()\n",
    "\n",
    "learner.fine_tune(epochs=3, cbs=WandbCallback())\n",
    "\n",
    "cleanup_gpu_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] AI4MARS: A dataset for Terrain-Aware autonomous driving on Mars. (2021, June 1). IEEE Conference Publication | IEEE Xplore. https://ieeexplore.ieee.org/document/9523149.\n",
    "\n",
    "[2] SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers. (2021, October 21). https://arxiv.org/abs/2105.15203v3.\n",
    "\n",
    "[2] Mish: A Self Regularized Non-Monotonic Activation Function. (2019, August 13). https://arxiv.org/abs/1908.08681"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4mars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
